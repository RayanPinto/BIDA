{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05f4e9a-7c7a-493e-9feb-1bf37653778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ee1370-e562-45a0-8125-170f781a1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0120363-1b5f-4503-87da-30c9bf4ec73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"Artificial intelligence/Artificialintelligence.html\", \"Startups_TechCrunch/Startups_TechCrunch.html\"]\n",
    "def extract(filepath):\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        soup=BeautifulSoup(f.read(),'html.parser')\n",
    "    for tag in soup(['script','noscript','style']): \n",
    "        tag.decompose()\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b821ae76-c101-458f-b093-5db673103871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "files =[\"Artificial intelligence/Artificialintelligence.html\", \"Startups_TechCrunch/Startups_TechCrunch.html\"]\n",
    "def extract(fp):\n",
    "    with open(fp,'r',encoding='utf-8') as f:\n",
    "        soup=BeautifulSoup(f.read(),'html.parser')\n",
    "    for tag in soup(['script','style','unscript']):\n",
    "            tag.decompose()\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "208c1bdc-d0b6-4f91-b5fe-3c38068a6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[ extract(ff) for ff in files if len(extract(ff))>=200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4d1bb86-7173-4a78-b7d7-c9030ecd2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words='english',min_df=1,max_df=0.9,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf0dc1e3-c3af-4c79-8b03-942278647aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "192bab75-9fa0-4ab8-9b11-5ea73b39fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "term=vectorizer.get_feature_names_out()\n",
    "scores=X.mean(axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5b52465-3755-414a-93ed-22c77603a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " days                           0.2366\n",
      " intelligence                   0.2248\n",
      " disrupt                        0.1873\n",
      " artificial                     0.1827\n",
      " artificial intelligence        0.1405\n",
      " techcrunch                     0.1183\n",
      " computer                       0.0984\n",
      " agotechcrunch disrupt          0.0887\n",
      " techcrunch disrupt             0.0887\n",
      " agotechcrunch                  0.0887\n",
      " 2025techcrunch                 0.0789\n",
      " disrupt 2025techcrunch         0.0789\n",
      " britannica                     0.0632\n",
      " days agotechcrunch             0.0591\n",
      " learning                       0.0562\n",
      " tasks                          0.0562\n",
      " startup                        0.0493\n",
      " history                        0.0422\n",
      " sep                            0.0422\n",
      " ability                        0.0422\n",
      " agoin                          0.0394\n",
      " app                            0.0394\n",
      " human                          0.0351\n",
      " knowledge                      0.0351\n",
      " ap                             0.0351\n"
     ]
    }
   ],
   "source": [
    "top_idx=scores.argsort()[::-1][:25]\n",
    "for i,j in enumerate(top_idx):\n",
    "    print(f\" {term[j]:30s} {scores[j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2af4754-0563-4ec0-b8fb-9a512fab21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=min(6,len(docs))\n",
    "nmf=NMF(n_components=k,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dd201a9-e052-43fd-80f6-a30baab50aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rayan\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_nmf.py:132: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(res * 2)\n"
     ]
    }
   ],
   "source": [
    "h=nmf.fit_transform(X)\n",
    "w=nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7fda082-35d8-4e0c-b2fe-6e670cbafcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics\n",
      "\n",
      " topic:0:days,disrupt,techcrunch,agotechcrunch disrupt,agotechcrunch,techcrunch disrupt,disrupt 2025techcrunch,2025techcrunch,days agotechcrunch,startup\n",
      " topic:1:intelligence,artificial,artificial intelligence,computer,britannica,tasks,learning,ability,history,sep\n"
     ]
    }
   ],
   "source": [
    "print(\"topics\\n\")\n",
    "for i,j in enumerate(w):\n",
    "    tw=j.argsort()[::-1][:10]\n",
    "    print(f\" topic:{i}:{ ','.join(term[tw])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c234a67-3135-4317-8d8d-4a5ad4afeaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Common Terms:\n",
      "express              30\n",
      "window               28\n",
      "opens                28\n",
      "august               21\n",
      "arrivals             17\n",
      "score                13\n",
      "research             12\n",
      "cricket              11\n",
      "visa                 11\n",
      "shorts               10\n",
      "continues            10\n",
      "screen               10\n",
      "wi                   10\n",
      "advertisement        10\n",
      "stem                 9\n",
      "\n",
      "Hidden Topics (LDA):\n",
      "Topic 1: express | window | opens | august | arrivals | score | cricket | shorts | wi | screen\n",
      "Topic 2: companies | united | visa | costs | states | emerging | tuition | remains | prestige | cost\n",
      "Topic 3: driven | director | fully | finance | ambition | living | ivy | investment | labs | contrast\n"
     ]
    }
   ],
   "source": [
    "# EXP8 - LDA Topic Modeling\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "\n",
    "# Files to process\n",
    "files = [\"The Indian Express/TheIndianExpress.html\", \"The Economic Times/TheEconomicTimes.html\", \"TIE/TIE.html\"]\n",
    "\n",
    "def extract_text(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    \n",
    "    # Remove scripts and styles\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # Extract text from paragraphs\n",
    "    text = \" \".join(p.get_text() for p in soup.find_all('p'))\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Extract texts\n",
    "texts = [extract_text(f) for f in files if len(extract_text(f)) > 50]\n",
    "\n",
    "# Create bag-of-words\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2, max_df=0.95)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Find common terms\n",
    "freq = X.sum(axis=0).A1\n",
    "top_idx = freq.argsort()[::-1][:15]\n",
    "print(\"Top Common Terms:\")\n",
    "for i, idx in enumerate(top_idx):\n",
    "    print(f\"{vocab[idx]:20s} {int(freq[idx])}\")\n",
    "\n",
    "# LDA Topic Modeling\n",
    "k = min(3, len(texts))\n",
    "lda = LatentDirichletAllocation(n_components=k, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "print(\"\\nHidden Topics (LDA):\")\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_words = topic.argsort()[::-1][:10]\n",
    "    print(f\"Topic {i+1}: {' | '.join(vocab[top_words])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "745c05f2-ad48-4c8b-bce1-447530f19099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Common Terms Across All Articles:\n",
      "express              30\n",
      "window               28\n",
      "opens                28\n",
      "august               21\n",
      "arrivals             17\n",
      "sports               14\n",
      "visa                 13\n",
      "research             12\n",
      "cricket              11\n",
      "tata                 10\n",
      "stem                 10\n",
      "shorts               10\n",
      "screen               10\n",
      "wi                   10\n",
      "continues            10\n",
      "\n",
      "Hidden Topics (LDA):\n",
      "Topic 1: 25 | ambition | budget | contrast | report | access | 60 | remote | sought | ivy\n",
      "Topic 2: express | opens | window | august | arrivals | sports | cricket | wi | screen | technology\n",
      "Topic 3: rate | visa | tata | united | companies | states | costs | weighing | tuition | emerging\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re, html, numpy as np\n",
    "\n",
    "files = [\"The Indian Express/TheIndianExpress.html\", \"The Economic Times/TheEconomicTimes.html\", \"TIE/TIE.html\"]\n",
    "\n",
    "texts = []\n",
    "for FILE in files:\n",
    "    with open(FILE, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        data = f.read()\n",
    "    s = BeautifulSoup(data, \"html.parser\")\n",
    "    for t in s([\"script\", \"style\"]):\n",
    "        t.decompose()\n",
    "    text = \" \".join(p.get_text(\" \", strip=True) for p in s.find_all(\"p\"))\n",
    "    text = html.unescape(re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text))\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if len(text) > 50:\n",
    "        texts.append(text)\n",
    "\n",
    "assert texts, \"No valid text found in the given articles.\"\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\", min_df=2, max_df=0.95)\n",
    "X = cv.fit_transform(texts)\n",
    "vocab = np.array(cv.get_feature_names_out())\n",
    "\n",
    "freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "top = freq.argsort()[::-1][:15]\n",
    "print(\"\\nTop Common Terms Across All Articles:\")\n",
    "for w, c in zip(vocab[top], freq[top]):\n",
    "    print(f\"{w:20s} {int(c)}\")\n",
    "\n",
    "k = min(3, len(texts))\n",
    "lda = LatentDirichletAllocation(n_components=k, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "print(\"\\nHidden Topics (LDA):\")\n",
    "for i, comp in enumerate(lda.components_):\n",
    "    terms = vocab[comp.argsort()[::-1][:10]]\n",
    "    print(f\"Topic {i+1}: {' | '.join(terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47324bc7-dd3e-4f9b-9a1c-b2ec117322a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a377386-ab45-47ab-b4f9-d3a24fefcfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
