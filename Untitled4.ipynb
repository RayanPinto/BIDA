{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb974ef-83e6-4df5-be97-cb75b9fa31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee3d3f4-bc44-447f-a701-b0020e820b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5a6757-b02d-4ca8-8b44-c889a942d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19fa2d76-26d8-47c9-abdb-c6ec8cc8f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"The Indian Express/TheIndianExpress.html\", \"The Economic Times/TheEconomicTimes.html\", \"TIE/TIE.html\"]\n",
    "def extract(fp):\n",
    "    with open(fp,'r',encoding='utf-8') as f:\n",
    "        soup=BeautifulSoup(f.read(),'html.parser')\n",
    "        for tag in soup(['script','style']):\n",
    "            tag.decompose()\n",
    "    txt=\" \".join(p.get_text() for p in soup.find_all('p'))\n",
    "    txt=re.sub(r\"https?://\\S+|www\\.\\S+\",\" \",txt)\n",
    "    txt=re.sub(r\"//\\s+\",\" \",txt).strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700c67be-fb83-47b1-9597-81c3a22cb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[ extract(p) for p in files if len(extract(p))>=50]\n",
    "vectorizer=CountVectorizer(stop_words='english',max_df=0.95,min_df=2)\n",
    "x=vectorizer.fit_transform(texts)\n",
    "terms=vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4294c8a5-e5e8-4e81-88e9-a8133e27a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " express 30.0\n",
      " window 28.0\n",
      " opens 28.0\n",
      " august 21.0\n",
      " arrivals 17.0\n",
      " score 13.0\n",
      " research 12.0\n",
      " cricket 11.0\n",
      " visa 11.0\n",
      " shorts 10.0\n",
      " continues 10.0\n",
      " screen 10.0\n",
      " wi 10.0\n",
      " advertisement 10.0\n",
      " stem 9.0\n"
     ]
    }
   ],
   "source": [
    "score=x.sum(axis=0).A1\n",
    "top_idx=score.argsort()[::-1][:15]\n",
    "for i,topic in enumerate(top_idx):\n",
    "    print(f\" {terms[topic]:.20s} {score[topic]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4527883-d5bd-4589-bf04-c136c28e75a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " topic:1 :companies| united| visa| costs| states| prestige| emerging| remains| cost| tuition\n",
      " topic:2 :driven| director| fully| finance| ambition| living| ivy| investment| labs| contrast\n",
      " topic:3 :express| window| opens| august| arrivals| score| cricket| advertisement| screen| shorts\n"
     ]
    }
   ],
   "source": [
    "k=min(6,len(texts))\n",
    "lda=LatentDirichletAllocation(n_components=k,random_state=42)\n",
    "lda.fit(x)\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    ll=topic.argsort()[::-1][:10]\n",
    "    print(f\" topic:{i+1} :{\"| \".join(terms[ll])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de5087d7-0307-44d4-9d6a-df2353b0b6d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "No comments found. Save a page with comments and set FILES.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m No comments found. Save a page with comments and set FILES.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rayan\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import html\n",
    "import os\n",
    "\n",
    "FILES = [r\"C:\\BIDA LAB\\EXP 7\\r_news.htm\"]\n",
    "\n",
    "LEX = {\n",
    "    \"good\": 2, \"great\": 3, \"excellent\": 4, \"amazing\": 4, \"love\": 3, \"like\": 2,\n",
    "    \"nice\": 2, \"awesome\": 4, \"helpful\": 2, \"bad\": -2, \"terrible\": -3, \"awful\": -3,\n",
    "    \"hate\": -3, \"slow\": -2, \"buggy\": -3, \"confusing\": -2, \"broken\": -3, \"issue\": -2,\n",
    "    \"problem\": -2, \"worst\": -4, \"disappointed\": -3, \"frustrating\": -3\n",
    "}\n",
    "\n",
    "NEG = {\"not\", \"no\", \"never\", \"none\", \"hardly\", \"barely\", \"scarcely\"}\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z][A-Za-z\\-']+\")\n",
    "URL_RE = re.compile(r\"https?://\\S+\")\n",
    "EMOJI_RE = re.compile(r\"[\\U00010000-\\U0010ffff]\")\n",
    "\n",
    "def clean(text):\n",
    "    text = html.unescape(text or \"\")\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = EMOJI_RE.sub(\"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return [w.lower() for w in WORD_RE.findall(text)]\n",
    "\n",
    "def score(text):\n",
    "    words = tokenize(text)\n",
    "    total = 0.0\n",
    "    for i, w in enumerate(words):\n",
    "        val = LEX.get(w, 0)\n",
    "        if val:\n",
    "            if any(words[i-j] in NEG for j in range(1, min(3, i) + 1)):\n",
    "                val *= -1\n",
    "            total += val\n",
    "    return total / max(1.0, math.log(len(words) + 1, 3))\n",
    "\n",
    "def grab_comments(html_doc):\n",
    "    soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "    for t in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"svg\"]):\n",
    "        t.decompose()\n",
    "    cands = soup.find_all(attrs={\"class\": re.compile(\"comment|reply\", re.I)})\n",
    "    cands += soup.find_all(id=re.compile(\"comment|reply\", re.I))\n",
    "    texts = []\n",
    "    for el in set(cands):\n",
    "        txt = clean(el.get_text(\" \", strip=True))\n",
    "        if len(txt) > 12:\n",
    "            texts.append(txt)\n",
    "    return list(dict.fromkeys(texts))\n",
    "\n",
    "comments = []\n",
    "for path in FILES:\n",
    "    if os.path.exists(path):\n",
    "        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            comments += grab_comments(f.read())\n",
    "\n",
    "comments = list(dict.fromkeys(comments))\n",
    "if not comments:\n",
    "    raise SystemExit(\"No comments found. Save a page with comments and set FILES.\")\n",
    "\n",
    "scores = [score(c) for c in comments]\n",
    "labels = []\n",
    "for s in scores:\n",
    "    if s >= 0.05:\n",
    "        labels.append(\"positive\")\n",
    "    elif s <= -0.05:\n",
    "        labels.append(\"negative\")\n",
    "    else:\n",
    "        labels.append(\"neutral\")\n",
    "\n",
    "cnt = Counter(labels)\n",
    "overall = max(cnt, key=cnt.get)\n",
    "print(\"Overall tone:\", overall)\n",
    "print(\"Counts:\", dict(cnt))\n",
    "\n",
    "order = [\"positive\", \"neutral\", \"negative\"]\n",
    "vals = [cnt.get(k, 0) for k in order]\n",
    "plt.bar(order, vals)\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42697312-ea2e-4484-8adb-67602f3e20eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No comments found\n"
     ]
    }
   ],
   "source": [
    "# EXP7 - HTML Sentiment Analysis\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Sentiment lexicon\n",
    "positive_words = {'good', 'great', 'excellent', 'amazing', 'love', 'like', 'nice', 'awesome', 'helpful', 'fantastic', 'wonderful', 'brilliant', 'outstanding', 'perfect', 'best', 'better', 'super', 'cool'}\n",
    "negative_words = {'bad', 'terrible', 'awful', 'hate', 'slow', 'buggy', 'confusing', 'broken', 'issue', 'problem', 'worst', 'disappointed', 'frustrating', 'annoying', 'horrible', 'disgusting', 'pathetic', 'useless', 'worthless', 'stupid', 'dumb', 'idiotic', 'ridiculous', 'absurd', 'nonsense', 'garbage', 'trash'}\n",
    "negation_words = {'not', 'no', 'never', 'none', 'hardly', 'barely', 'scarcely'}\n",
    "\n",
    "def extract_comments(html_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    \n",
    "    # Remove scripts and styles\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # Find comment elements\n",
    "    comments = []\n",
    "    for elem in soup.find_all(['div', 'p', 'span', 'article']):\n",
    "        if any(word in elem.get('class', []) for word in ['comment', 'reply', 'review']):\n",
    "            text = elem.get_text().strip()\n",
    "            if len(text) > 12:\n",
    "                comments.append(text)\n",
    "    \n",
    "    return comments\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    score = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if word in positive_words:\n",
    "            score += 1\n",
    "        elif word in negative_words:\n",
    "            score -= 1\n",
    "        \n",
    "        # Check for negation\n",
    "        if i > 0 and words[i-1] in negation_words:\n",
    "            score *= -1\n",
    "    \n",
    "    return \"positive\" if score > 0 else (\"negative\" if score < 0 else \"neutral\")\n",
    "\n",
    "# Process file\n",
    "comments = extract_comments(\"r_news/r_news.html\")\n",
    "if not comments:\n",
    "    print(\"No comments found\")\n",
    "else:\n",
    "    sentiments = [analyze_sentiment(comment) for comment in comments]\n",
    "    counts = Counter(sentiments)\n",
    "    \n",
    "    print(f\"Overall tone: {max(counts, key=counts.get)}\")\n",
    "    print(f\"Counts: {dict(counts)}\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nSample comments:\")\n",
    "    for i, comment in enumerate(comments[:5]):\n",
    "        print(f\"{i+1}. [{sentiments[i]}] {comment[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58951597-93a4-49b5-bbec-a57e09223d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No comments found. Try saving a webpage with visible user comments.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# ----------- Input File -----------\n",
    "file_path = r\"r_news.html\"\n",
    "\n",
    "# ----------- Basic Sentiment Words -----------\n",
    "positive_words = [\"good\",\"great\",\"excellent\",\"amazing\",\"love\",\"like\",\"nice\",\"awesome\",\"helpful\",\"best\"]\n",
    "negative_words = [\"bad\",\"terrible\",\"awful\",\"hate\",\"slow\",\"buggy\",\"confusing\",\"broken\",\"issue\",\"worst\"]\n",
    "\n",
    "# ----------- Extract Comments from HTML -----------\n",
    "def extract_comments(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "    comments = []\n",
    "    \n",
    "    # Find any tag with class having \"comment\" or \"reply\"\n",
    "    for tag in soup.find_all(True, class_=re.compile(\"comment|reply\", re.I)):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if len(text) > 10:\n",
    "            comments.append(text)\n",
    "\n",
    "    return comments\n",
    "\n",
    "# ----------- Simple Sentiment Function -----------\n",
    "def get_sentiment(text):\n",
    "    words = text.lower().split()\n",
    "    score = 0\n",
    "    \n",
    "    for w in words:\n",
    "        if w in positive_words:\n",
    "            score += 1\n",
    "        if w in negative_words:\n",
    "            score -= 1\n",
    "\n",
    "    if score > 0:\n",
    "        return \"positive\"\n",
    "    elif score < 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# ----------- Run Analyzer -----------\n",
    "comments = extract_comments(file_path)\n",
    "\n",
    "if not comments:\n",
    "    print(\"No comments found. Try saving a webpage with visible user comments.\")\n",
    "else:\n",
    "    sentiments = [get_sentiment(c) for c in comments]\n",
    "    counts = Counter(sentiments)\n",
    "\n",
    "    print(\"Overall tone:\", max(counts, key=counts.get))\n",
    "    print(\"Counts:\", dict(counts))\n",
    "\n",
    "    # ----------- Bar Plot -----------\n",
    "    labels = [\"positive\",\"neutral\",\"negative\"]\n",
    "    values = [counts.get(l,0) for l in labels]\n",
    "\n",
    "    plt.bar(labels, values)\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.ylabel(\"Number of Comments\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196725b3-0e3d-4fbd-8d04-41eab325fb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
